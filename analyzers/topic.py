import spacy
from gensim.models import CoherenceModel, LdaModel
from gensim.corpora import Dictionary
from gensim.models.phrases import Phrases, Phraser
from nltk.tokenize import sent_tokenize
import numpy as np
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.model_selection import train_test_split

class SophisticatedTopicAnalyzer:
    """
    Dynamic topic modeling with automatic optimization:
    1. Determines optimal number of topics using coherence scores
    2. Processes n-grams for better phrase capture
    3. Implements hierarchical topic structure
    4. Handles topic evolution and stability
    """
    
    def __init__(self, min_topics=2, max_topics=10):
        self.nlp = spacy.load('en_core_web_sm')
        self.min_topics = min_topics
        self.max_topics = max_topics
        self.optimal_model = None
        self.dictionary = None
        self.corpus = None
    
    def preprocess_text(self, texts):
        """Preprocess texts for topic modeling"""
        processed_texts = []
        for text in texts:
            # Tokenize and clean text using spaCy
            doc = self.nlp(text.lower())
            # Keep only content words (nouns, verbs, adjectives, adverbs)
            tokens = [token.lemma_ for token in doc 
                     if not token.is_stop and not token.is_punct 
                     and token.pos_ in {'NOUN', 'VERB', 'ADJ', 'ADV'}]
            processed_texts.append(tokens)
        
        # Build bigram and trigram models
        bigram = Phrases(processed_texts, min_count=5, threshold=100)
        trigram = Phrases(bigram[processed_texts], threshold=100)
        
        # Get phrase models
        bigram_mod = Phraser(bigram)
        trigram_mod = Phraser(trigram)
        
        # Apply phrase models
        final_texts = []
        for tokens in processed_texts:
            trigrams = trigram_mod[bigram_mod[tokens]]
            final_texts.append(trigrams)
        
        return final_texts
    
    def find_optimal_topics(self, texts, coherence='c_v', max_iterations=100):
        """
        Identifies ideal topic count using coherence optimization.
        
        Implementation based on:
        - RÃ¶der et al. (2015): Exploring the space of topic coherence measures
        - Mimno et al. (2011): Optimizing semantic coherence in topic models
        
        Args:
            texts: List of preprocessed documents
            coherence: Coherence measure ('c_v', 'u_mass', or 'c_uci')
            max_iterations: Maximum LDA iterations
        
        Returns:
            Dict containing:
            - optimal_topics: Optimal number of topics
            - coherence_scores: List of coherence scores
            - optimal_coherence: Best coherence score
        """
        processed_texts = self.preprocess_text(texts)
        
        # Create dictionary and corpus
        self.dictionary = Dictionary(processed_texts)
        self.corpus = [self.dictionary.doc2bow(text) for text in processed_texts]
        
        # Store coherence values
        coherence_values = []
        models = []
        
        # Implement held-out likelihood estimation following Wallach et al. (2009)
        held_out_ratio = 0.2
        train_corpus, test_corpus = train_test_split(self.corpus, test_size=held_out_ratio)
        
        for num_topics in range(self.min_topics, self.max_topics + 1):
            # Train LDA model
            model = LdaModel(
                corpus=train_corpus,
                num_topics=num_topics,
                id2word=self.dictionary,
                iterations=max_iterations,
                passes=5
            )
            
            # Calculate coherence score
            coherence_model = CoherenceModel(
                model=model,
                texts=processed_texts,
                dictionary=self.dictionary,
                coherence=coherence
            )
            coherence_score = coherence_model.get_coherence()
            
            coherence_values.append(coherence_score)
            models.append(model)
        
        # Find optimal number of topics
        optimal_idx = np.argmax(coherence_values)
        self.optimal_model = models[optimal_idx]
        
        return {
            'optimal_topics': self.min_topics + optimal_idx,
            'coherence_scores': coherence_values,
            'optimal_coherence': coherence_values[optimal_idx]
        }
    
    def analyze_topics(self, texts):
        """Analyze topics in the given texts"""
        if not self.optimal_model:
            optimal_results = self.find_optimal_topics(texts)
        
        # Get topic distribution for each document
        processed_texts = self.preprocess_text(texts)
        corpus = [self.dictionary.doc2bow(text) for text in processed_texts]
        
        # Calculate topic diversity
        topic_distributions = []
        for doc in corpus:
            doc_topics = self.optimal_model.get_document_topics(doc)
            topic_dist = [0] * self.optimal_model.num_topics
            for topic_id, prob in doc_topics:
                topic_dist[topic_id] = prob
            topic_distributions.append(topic_dist)
        
        # Calculate topic diversity using entropy
        topic_diversity = np.mean([
            -sum(p * np.log2(p) if p > 0 else 0 for p in dist)
            for dist in topic_distributions
        ])
        
        return {
            'topic_diversity': topic_diversity,
            'topic_coherence': optimal_results['optimal_coherence'],
            'num_topics': optimal_results['optimal_topics'],
            'topic_distributions': topic_distributions
        }